{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook loads a pre-trained HyperLightspeedBench (HLB) neural network for image classification on CIFAR-10 dataset and experiments with low-precision low-rank approximation of the weight matrices of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "sys.path.extend([os.path.abspath(\"src\")])\n",
    "\n",
    "from math import ceil, floor\n",
    "from typing import Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hlb.speedyresnet import (\n",
    "    SpeedyResNet,\n",
    "    Conv,\n",
    "    ConvGroup,\n",
    "    TemperatureScaler,\n",
    "    Linear,\n",
    "    BatchNorm,\n",
    "    FastGlobalMaxPooling,\n",
    ")\n",
    "from src.hlb.config import hyp\n",
    "from src.hlb.utils import get_batches\n",
    "\n",
    "from src.lplr.quantizers import quantize\n",
    "from src.lplr.compressors import lplr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_models(model: nn.Module, benchmark_model):\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = model.to(device).float()\n",
    "    benchmark_model = benchmark_model.to(device).float()\n",
    "    data = torch.load(hyp[\"misc\"][\"data_location\"], map_location=device)\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.2, reduction=\"none\")\n",
    "\n",
    "    eval_batchsize = 2500\n",
    "    from collections import defaultdict\n",
    "    loss_list_val, acc_list = defaultdict(list), defaultdict(list)\n",
    "\n",
    "    model_types = (\"Test model\", \"Benchmark Model\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in get_batches(\n",
    "            data, key=\"eval\", batchsize=eval_batchsize\n",
    "        ):\n",
    "            \n",
    "            input_tensors = inputs.float()\n",
    "            \n",
    "            for mm, mt in zip((model, benchmark_model), model_types):\n",
    "                \n",
    "                outputs = mm(input_tensors)\n",
    "                loss_val = loss_fn(outputs, targets).float().mean()\n",
    "                acc_val = (outputs.argmax(-1) == targets.argmax(-1)).float().mean()\n",
    "                logger.trace(f\"loss {loss_val:.3f} acc {acc_val:.3f} for {mt}\")\n",
    "            \n",
    "                loss_list_val[mt].append(loss_val)\n",
    "                acc_list[mt].append(acc_val)\n",
    "\n",
    "    for mt in model_types:\n",
    "        avg_val_acc = torch.mean(torch.tensor(acc_list[mt])).item()\n",
    "        avg_val_loss = torch.mean(torch.tensor(loss_list_val[mt])).item()\n",
    "\n",
    "        logger.debug(f\"Avg Validation Accuracy: {avg_val_acc:.2f} and Avg Validation Loss {avg_val_loss} for {mt}\")\n",
    "    return (torch.mean(torch.tensor(acc_list[\"Test model\"])).item(), torch.mean(torch.tensor(loss_list_val[\"Test model\"])).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for (name, param) in model.named_parameters():\n",
    "#     print(name, param.shape, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_param = model.get_parameter(\"net_dict.initial_block.whiten.weight\").to(\"cpu\").detach().numpy()\n",
    "# param_shape = model_param.shape\n",
    "# reshaped_param = model_param.reshape(param_shape[0], param_shape[1], -1)\n",
    "# rp_copy = np.copy(reshaped_param)\n",
    "# for idxs in range(reshaped_param.shape[-1]):\n",
    "#     P = reshaped_param[:, :, idxs]\n",
    "#     # P = np.interp(P, (P.min(), P.max()), (-1, 1))\n",
    "#     rp_copy[:, :, idxs] = lplr(P, 3, 32, 32)\n",
    "\n",
    "# rp_final = rp_copy.reshape(param_shape)\n",
    "\n",
    "# errs = []\n",
    "# for row in range(param_shape[-2]):\n",
    "#     for col in range(param_shape[-1]):\n",
    "#         errs.append(error(model_param[:, :, row, col], rp_final[:, :, row, col]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantize_layers(model: nn.Module, compressor: Callable[[np.ndarray], np.ndarray] = lplr) -> nn.Module:\n",
    "    from math import ceil, floor\n",
    "    from copy import deepcopy\n",
    "\n",
    "    output_model = deepcopy(model)\n",
    "    # b1 = 8\n",
    "    # b2 = 8\n",
    "    # frac = 0.9\n",
    "    for name, param in output_model.named_parameters():\n",
    "        model_param = param.to(\"cpu\").detach().numpy()\n",
    "        param_shape = model_param.shape\n",
    "        logger.trace(f\"Applying LPLR on {name} with shape {param_shape}\")\n",
    "        if param.ndim >= 2:\n",
    "            reshaped_param = model_param.reshape(param_shape[0], param_shape[1], -1)\n",
    "            out_param = np.zeros_like(reshaped_param)\n",
    "            for idxs in range(reshaped_param.shape[-1]):\n",
    "                # out_param[:, :, idxs] = compressor(\n",
    "                #     reshaped_param[:, :, idxs],\n",
    "                #     ceil(frac * rank),\n",
    "                #     b1,\n",
    "                #     b2,\n",
    "                # )\n",
    "                out_param[:, :, idxs] = compressor(reshaped_param[:, :, idxs])\n",
    "            param.data = torch.from_numpy(out_param.reshape(param_shape))\n",
    "        elif param.ndim == 1:\n",
    "            param.data = torch.from_numpy(quantize(model_param))\n",
    "    return output_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_location = \"artifacts/hlb/checkpoints/trained-speedyresnet.pt\"\n",
    "model = torch.load(model_location, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pname = \"net_dict.initial_block.whiten.weight\"\n",
    "# torch.linalg.norm(quantized_model.get_parameter(pname) - model.get_parameter(pname)) / torch.linalg.norm(model.get_parameter(pname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def comp(param, fraction):\n",
    "    from math import ceil\n",
    "    rank = np.min(param.shape)\n",
    "    try:\n",
    "        out_rank = ceil(fraction * rank)\n",
    "        assert isinstance(out_rank, int)\n",
    "    except AssertionError as ae:\n",
    "        logger.error(f\"Wrong Out rank: {out_rank}\")\n",
    "        raise ae\n",
    "    logger.debug(f\"Shape: {param.shape} Input Rank: {rank} Output Rank: {out_rank}\")\n",
    "    return lplr(param, out_rank, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quantized_models = {f: quantize_layers(model, compressor=partial(comp, fraction=float(f))) for f in (\"0.9\", \"0.99\", \"0.999\", \"1.0\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, qm in quantized_models.items():\n",
    "    (acc, loss) = evaluate_models(qm, model)\n",
    "    logger.info(f\"Computing fraction {f} with accuracy {acc:.3f} and loss {loss:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
